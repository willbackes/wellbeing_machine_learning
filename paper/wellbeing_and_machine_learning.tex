\documentclass[11pt, a4paper, leqno]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
% \usepackage{chngcntr}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm}
\usepackage{caption}
\usepackage{mdwlist}
\usepackage{xfrac}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{minibox}
\usepackage{csvsimple}
\usepackage{booktabs}
\usepackage{pgfplotstable}

% \usepackage{pdf14} % Enable for Manuscriptcentral -- can't handle pdf 1.5
% \usepackage{endfloat} % Enable to move tables / figures to the end. Useful for some
% submissions.

\usepackage[
    natbib=true,
    bibencoding=inputenc,
    bibstyle=authoryear-ibid,
    citestyle=authoryear-comp,
    maxcitenames=3,
    maxbibnames=10,
    useprefix=false,
    sortcites=true,
    backend=biber
]{biblatex}
\AtBeginDocument{\toggletrue{blx@useprefix}}
\AtBeginBibliography{\togglefalse{blx@useprefix}}
\setlength{\bibitemsep}{1.5ex}
\addbibresource{refs.bib}

\usepackage[unicode=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    citecolor=NavyBlue,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=NavyBlue
}


\widowpenalty=10000
\clubpenalty=10000

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}
\setstretch{1.5}


\begin{document}

\title{Comparative Analysis of Predictive Models on Human Wellbeing\thanks{William Backes, University of Bonn. Email: \href{mailto:s19wback@uni-bonn.de}{\nolinkurl{s19wback [at] uni-bonn [dot] de}}.}}

\author{William Backes}

\date{
    {\bf Preliminary -- please do not quote}
    \\[1ex]
    \today
}

\maketitle




\clearpage


\section{Introduction} % (fold)
\label{sec:introduction}

This project is centered around exploring the application of machine learning techniques
to predict human wellbeing, comparing traditional econometric methods, such as Ordinary
Least Squares (OLS), with modern algorithms like Least Absolute Shrinkage and Selection
Operator (LASSO), Random Forests (RF), and Gradient Boosting (GB). Inspired by the work
of Oparina et al. (2023), the aim is to assess the effectiveness of various models in
predicting human wellbeing. The project was created using the template by
\citet{GaudeckerEconProjectTemplates}.
% section introduction (end)

\section{Methods} % (fold)
\label{sec:methods}

\subsection{Data}
The dataset for this project is sourced from the German Socio-Economic Panel (SOEP),
covering the years 2010 to 2018. This timeframe aligns with the original paper, and
flexibility is maintained to consider other years as long as data remains available.

The SOEP is a representative longitudinal survey of private households in Germany.
One main feature of the SOEP data is that it follows the same private households,
individuals, and families every year, surveying a range of topics from living conditions,
education to values and personality since 1984.

Wellbeing is measured by the self-reported variable Life Satisfaction on a scale from 0
to 10, where 0 represents the lowest and 10 the highest level of satisfaction.
Figure~\ref{fig:histogram_life_satisfaction} shows the distribution of life satisfaction
in the dataset. This figure is comparable to the Appendix Figure A1 in the original paper
and was created using the \texttt{plotly} package in Python.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../bld/final/histogram_life_satisfaction}
    \caption{Histograms of life satisfaction for SOEP data.}
    \label{fig:histogram_life_satisfaction}
\end{figure}

\subsection{Algorithms}

The algorithms used in this project are OLS, LASSO, Random Forest, and Gradient Boosting.
The Python package \texttt{scikit-learn} is used to implement the models. The Random Forest
and Gradient Boosting models are trained on the training set and evaluated on the test set.
The training set contains 80\% of the data, and the test set contains the remaining 20\%.
In order to reduce the amount of time spent on training the models, cross-validation is not
used to find the optimal hyperparameters. However, the functions were written in a way that
allow to use grid search to find the optimal hyperparameters.

The performance of the models is measured using the $R^2$ score, which is the proportion of
the variance in the dependent variable that is predictable from the independent variables.
To evaluate the importance of the variables, the permutation importance is used. This method
measures the increase in the prediction error of the model after permuting the feature. The
larger the increase, the more important the feature is.


\subsection{Explanatory variables}
In this project, focus was placed on the restricted set of variables used in the original
paper. This set includes: sex, age, age-squared, ethnicity, religiosity, number of household
members, number of children in the household, marital status, log household income, general
health status, disability, body mass index, labour-force status, working hours, home ownership,
area of residence, and interview month. The variables are grouped into two categories
(household and individual level data) and include cotinuous, categorical and binary variables.
The descriptive statistics of the continuous variables are shown in Table~\ref{tab:descriptive_stats_continuous}
and is comparable to the Table A2 in the original paper.

\begin{table}[htbp]
    \centering
    \input{../bld/final/descriptive_stats_continuous.tex}
    \caption{List of continuous variables in the restricted set: descriptive statistics.}
    \label{tab:descriptive_stats_continuous}
\end{table}


Note that the values differ for most of the variables, but more significantly for the Log HH Income,
Health, and Working hours. This is due to the fact that the SOEP data contains an extensive
set of variables, with some of them being similar to each other. For example, the variable Health
measures the number of doctor visits in previous year. In SOEP data, there are the variables
Total Number Of Visits To Doctor (ple0059), Number Of Visits To Other Doctor (ple0070), Number of
annual doctor visits (m11127), Number Of Doctor Visits Last Three Mths (ple0072), and others.
As the original paper does not specify which variable was used and in the case I could not find
the exact variable, I used the variable that included the most observations. This process
demanded a careful examination of the variables and the data, and the results are comparable
to some extent to the original paper.



\section{Results} % (fold)
\label{sec:results}

\subsection{Model performance}

To evaluate the performance of the models, the $R^2$ score is used. Figure~\ref{fig:r_squared_algorithms}
depicts the $R^2$ scores mean over the years 2010 to 2018 for the different models.The
results are comparable to the Figure 1 Panel A in the original paper.

In this figure, it is possible to see that the Random Forest and Gradient Boosting models
do not outperform the OLS and Lasso models, in contrast to the original paper. The results
are driven by the fact that the models are not optimized, and the hyperparameters are not
tuned. When using grid search to find the optimal hyperparameters, the Random Forest and
Gradient Boosting models outperform the OLS and Lasso models, as shown in the original paper.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../bld/final/r_squared_algorithms}
    \caption{Model performances as measured by R-squared.}
    \label{fig:r_squared_algorithms}
\end{figure}


\subsection{Variable importance}

To assess the importance of the variables, the permutation importance is used. This metric
measures the degree to which each algorithm relies on each variable in making its predictions.
The results are shown in Table~\ref{tab:permutation_importance_table}. The table is comparable
to the Table A5 in the original paper, and the results show the 10 most-important variables
for each model.

Anayzing the results, it is possible to see that the most important variables are more or less
consistent across the models, and definitely consistent with the original paper. The most
important variables to predict wellbeing are: age and age squared, log household income,
health and disability status.



\begin{table}[htbp]
    \centering
    \input{../bld/final/permutation_importance_table.tex}
    \caption{Permutation Importance (PI) in OLS, Lasso, RF and GB on the Restricted Set of variables: the 10 most-important variables.}
    \label{tab:permutation_importance_table}
\end{table}


\subsection{Wellbeing by age and income}

Figure~\ref{fig:average_wellbeing_by_income} and Figure~\ref{fig:average_wellbeing_by_age} depict
average predicted wellbeing for different levels of age and income, holding the other covariates
constant. Whether the relationship between age and wellbeing is U-shaped, and whether there is a
satiation point beyond which income no longer yields wellbeing are two open and hotly debated questions.
Tree-based algorithms freely estimate the most-appropriate functional forms. They are thus
particularly well-suited to act as agnostic judges in these debates.

The results are comparable to the Figure 3 Panel A in the original paper, and show that the
relationship between age and wellbeing is U-shaped, specially for the OLS and Lasso models.
For the Random Forest and Gradient Boosting models, the U-shape is not that evident, but the result
is similar to the original paper. In terms of income, the relationship is logarithmic, and there
is a point where the increase in income does not yield an increase in wellbeing for all models,
but specially for the Gradient Boosting model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../bld/final/average_wellbeing_by_income}
    \caption{Wellbeing and household income: restricted set of variables.}
    \label{fig:average_wellbeing_by_income}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../bld/final/average_wellbeing_by_age}
    \caption{Wellbeing and age: restricted set of variables.}
    \label{fig:average_wellbeing_by_age}
\end{figure}



\section{Discussion} % (fold)
\label{sec:discussion}

To some extent, the results of the models are comparable to the original paper. The models
are able to predict wellbeing, and the most important variables are consistent across the
models. However, the Random Forest and Gradient Boosting models do not outperform the OLS
and Lasso models, as shown in the original paper. This is due to the fact that the models
are not optimized, and the hyperparameters are not tuned.

The most important explanatory variables used in the models are consistent with the original paper.
The relationship between age and wellbeing is U-shaped, and the relationship between income and
wellbeing is logarithmic.

If the specific variables used in the original paper were available, the results would be
more similar to the original paper. However, the SOEP data contains an extensive set of variables,
and some of them are similar to each other. This demanded a careful examination of the variables
and the data, which required a considerable amount of time.






\setstretch{1}
\printbibliography
\setstretch{1.5}


% \appendix

% The chngctr package is needed for the following lines.
% \counterwithin{table}{section}
% \counterwithin{figure}{section}

\end{document}
